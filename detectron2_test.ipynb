{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, os\n",
    "import time\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, transforms, build_detection_test_loader\n",
    "from detectron2.engine import DefaultTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_cracks_dicts(img_dir):\n",
    "    dataset_dicts = []\n",
    "    image_paths = list(sorted(os.listdir(img_dir + 'image/')))\n",
    "    mask_paths = None\n",
    "    if os.path.exists(img_dir+'mask/'):\n",
    "        mask_paths = list(sorted(os.listdir(img_dir+'mask/')))\n",
    "    for idx, v in enumerate(image_paths):\n",
    "        record = {}\n",
    "        \n",
    "        filename = os.path.join(img_dir, 'image/', v)\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "        \n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "\n",
    "        if mask_paths:\n",
    "            mask_path = img_dir + \"mask/\" + mask_paths[idx]\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # HxWxC\n",
    "            contours,_=cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) \n",
    "\n",
    "            objs = []\n",
    "            for cnt in contours : \n",
    "                approx = cv2.approxPolyDP(cnt, 0.009 * cv2.arcLength(cnt, True), True) \n",
    "                if approx.shape[0] < 3:\n",
    "                    continue\n",
    "                # cv2.drawContours(image, [approx], 0, (0, 0, 255), 1) \n",
    "                # get bounding box coordinates for each mask\n",
    "                px = [pos[0][0] for pos in approx]\n",
    "                py = [pos[0][1] for pos in approx]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "                obj = {\n",
    "                    \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                    \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                    \"segmentation\": [poly],\n",
    "                    \"category_id\": 0,\n",
    "                    \"iscrowd\": 0\n",
    "                }\n",
    "                objs.append(obj)\n",
    "            record[\"annotations\"] = objs\n",
    "            record[\"sem_seg_file_name\"] = mask_path # for segmentation evaluator\n",
    "        else:\n",
    "            record[\"annotations\"] = [{\n",
    "                    \"bbox\": [0, 0, 0, 0],\n",
    "                    \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                    \"segmentation\": [(0, 0), (0, 0), (0, 0)],\n",
    "                    \"category_id\": 0,\n",
    "                    \"iscrowd\": 0\n",
    "                }]\n",
    "            record[\"sem_seg_file_name\"] = '' # for segmentation evaluator\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dir = \"/home/aicenter/Documents/hsu/data/CrackForest-dataset/test/\"\n",
    "# test_dir = \"/home/aicenter/Documents/hsu/data/test_dataset/test_ceci/640xh/result_256x256/crack/\"\n",
    "# test_dir = \"/home/aicenter/Documents/hsu/data/test_dataset/test_ceci/640xh/result_256x256/noncrack/\"\n",
    "# test_dir = \"/home/aicenter/Documents/hsu/data/test_dataset/test_ceci/640xh/256x256/\"\n",
    "# test_dir = \"/home/aicenter/Documents/hsu/data/test_dataset/test_ceci/640xh/256x256_Trans/\"\n",
    "test_dir = \"/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/\"\n",
    "# test_dir = \"/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets_Trans/\"\n",
    "# test_dir = \"/home/aicenter/Documents/hsu/data/test_dataset/test_seg/\"\n",
    "# test_dir = \"/home/aicenter/Documents/hsu/data/test_dataset/test_seg_Trans/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "namespace(name='cracks_test',\n          thing_classes=['crack'],\n          thing_colors=[(96, 151, 255)])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "DatasetCatalog.register(\"cracks_test\", lambda d= \"test\": get_cracks_dicts(test_dir))\n",
    "MetadataCatalog.get(\"cracks_test\").set(thing_classes=[\"crack\"], thing_colors=[(96, 151, 255)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (crack)\n",
    "cfg.MODEL.WEIGHTS = \"./models/mask_101_concrete_50000_0526.pth\"\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"cracks_test\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_0.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_1.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_10.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_100.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_101.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_102.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_103.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_104.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_11.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_2.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_20.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_21.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_22.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_23.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_24.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_25.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_26.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_27.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_28.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_29.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_3.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_30.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_31.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_32.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_33.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_34.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_35.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_36.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_37.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_38.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_39.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_4.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_40.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_41.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_42.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_43.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_44.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_45.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_46.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_47.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_48.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_49.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_5.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_50.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_51.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_52.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_53.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_54.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_55.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_56.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_57.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_58.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_59.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_6.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_60.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_61.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_62.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_63.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_64.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_65.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_66.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_67.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_68.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_69.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_7.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_70.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_71.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_72.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_73.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_74.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_75.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_76.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_77.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_78.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_79.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_8.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_80.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_81.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_82.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_83.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_84.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_85.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_86.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_87.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_88.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_89.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_9.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_90.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_91.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_92.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_93.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_94.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_95.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_96.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_97.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_98.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_99.png\n"
    }
   ],
   "source": [
    "# DRAW ONLY SEGEMENTATION\n",
    "is_trans = 'Trans' in test_dir\n",
    "root, folder_name = '', ''\n",
    "if is_trans:\n",
    "    split_path = file_name.split('/')\n",
    "    folder_name = '_'.join(split_path[-3].split('_')[:-1])\n",
    "    root = '/'.join(split_path[:-3])\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "mask_color = np.array((0, 255, 0), dtype=\"uint8\")\n",
    "for idx, d in enumerate(dataset_dicts):\n",
    "    file_name = d[\"file_name\"]\n",
    "    im = cv2.imread(file_name)\n",
    "    if idx%20==0:\n",
    "        print(file_name)\n",
    "    outputs = predictor(im)\n",
    "    \n",
    "    image_name = file_name.split('/')[-1]\n",
    "    if is_trans:\n",
    "        original_path = '%s/%s/image/%s'%(root, folder_name, image_name)\n",
    "        im = cv2.imread(original_path)\n",
    "    masks = outputs['instances'].pred_masks.cpu().numpy()\n",
    "    masks = masks[outputs['instances'].scores.cpu().numpy() > cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST]\n",
    "    mask = np.sum(masks, axis=0) > 0\n",
    "    roi = im[mask]\n",
    "    im[mask] = ((0.5 * mask_color) + (0.5 * roi)).astype(\"uint8\")\n",
    "    # save_dir = './result/segmentation/'\n",
    "    save_dir = test_dir + '/result/'\n",
    "    cv2.imwrite(save_dir + file_name.split('/')[-1], im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_0.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_1.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_10.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_100.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_101.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_102.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_103.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_104.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_11.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_2.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_20.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_21.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_22.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_23.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_24.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_25.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_26.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_27.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_28.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_29.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_3.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_30.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_31.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_32.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_33.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_34.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_35.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_36.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_37.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_38.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_39.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_4.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_40.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_41.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_42.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_43.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_44.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_45.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_46.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_47.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_48.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_49.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_5.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_50.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_51.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_52.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_53.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_54.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_55.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_56.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_57.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_58.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_59.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_6.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_60.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_61.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_62.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_63.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_64.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_65.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_66.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_67.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_68.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_69.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_7.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_70.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_71.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_72.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_73.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_74.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_75.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_76.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_77.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_78.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_79.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_8.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_80.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_81.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_82.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_83.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_84.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_85.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_86.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_87.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_88.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_89.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_9.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_90.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_91.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_92.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_93.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_94.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_95.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_96.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_97.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_98.png\n/home/aicenter/Documents/hsu/data/test_dataset/tw_labels/datasets/image/crop_99.png\n"
    }
   ],
   "source": [
    "# DRAW ALL\n",
    "is_trans = 'Trans' in test_dir\n",
    "root, folder_name = '', ''\n",
    "if is_trans:\n",
    "    split_path = file_name.split('/')\n",
    "    folder_name = '_'.join(split_path[-3].split('_')[:-1])\n",
    "    root = '/'.join(split_path[:-3])\n",
    "\n",
    "save_dir = test_dir + '/result/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "for d in dataset_dicts:\n",
    "    file_name = d[\"file_name\"]\n",
    "    im = cv2.imread(file_name)\n",
    "    print(file_name)\n",
    "    outputs = predictor(im)\n",
    "    \n",
    "    image_name = file_name.split('/')[-1]\n",
    "    if is_trans:\n",
    "        original_path = '%s/%s/image/%s'%(root, folder_name, image_name)\n",
    "        im = cv2.imread(original_path)\n",
    "        \n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=crack_metadata, \n",
    "                   scale=1, \n",
    "                   instance_mode=ColorMode.SEGMENTATION   # remove the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    # save_dir = './result/segmentation/'\n",
    "    cv2.imwrite(save_dir + file_name.split('/')[-1], v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saved at /home/aicenter/Documents/hsu/data/test_dataset/test_ceci/640xh/gt_640_seg.png\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "crop_w, crop_h = 256, 256\n",
    "ceci_dir = '/home/aicenter/Documents/hsu/data/test_dataset/test_ceci/640xh/%sx%s/image/' %(crop_w, crop_h)\n",
    "source_path = '/home/aicenter/Documents/hsu/data/test_dataset/test_ceci/640xh/gt_640.png'\n",
    "\n",
    "height, width, _ = cv2.imread(source_path).shape\n",
    "x_num, y_num = int(width/crop_w), int(height/crop_h)\n",
    "\n",
    "mask_color = np.array((0, 0, 255), dtype=\"uint8\") \n",
    "whole_image = np.array([])\n",
    "v_im = np.array([])\n",
    "for filename in sorted(os.listdir(ceci_dir)):\n",
    "    count_id = int(filename.split('.')[0])\n",
    "    image_path = ceci_dir+filename\n",
    "    im = cv2.imread(image_path)\n",
    "    outputs = predictor(im)\n",
    "    \n",
    "    masks = outputs['instances'].pred_masks.cpu().numpy()\n",
    "    # masks = masks[outputs['instances'].scores.cpu().numpy() > cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST]\n",
    "    masks = masks[outputs['instances'].scores.cpu().numpy() > 0.5]\n",
    "    mask = np.sum(masks, axis=0) > 0\n",
    "    roi = im[mask]\n",
    "    im[mask] = ((0.5 * mask_color) + (0.5 * roi)).astype(\"uint8\")\n",
    "    if (count_id+1)%y_num == 0:\n",
    "        whole_image = v_im if whole_image.size == 0 else np.hstack((whole_image, v_im))\n",
    "        v_im = np.array([])\n",
    "    else:\n",
    "        v_im = im if v_im.size == 0 else np.vstack((v_im, im))\n",
    "\n",
    "save_path = source_path.split('.png')[0] + '_seg.png'\n",
    "print('Saved at', save_path)\n",
    "cv2.imwrite(save_path, whole_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation.sem_seg_evaluation import SemSegEvaluator\n",
    "from detectron2.evaluation import inference_on_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir = '/home/aicenter/Documents/hsu/data/validation/'\n",
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(\"cracks_val\", lambda d= \"val\": get_cracks_dicts(val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cfg = get_cfg()\n",
    "val_cfg.DATALOADER.NUM_WORKERS = 2\n",
    "val_cfg.MODEL.WEIGHTS = \"./models/mask_101_concrete_50000_0526.pth\"\n",
    "val_cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 300\n",
    "val_cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 \n",
    "val_cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (crack)\n",
    "val_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set the testing threshold for this model\n",
    "cfg.DATASETS.TRAIN = (\"crack_train\",)\n",
    "val_cfg.DATASETS.TEST = (\"cracks_val\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[05/27 17:51:49 d2.engine.defaults]: \u001b[0mModel:\nGeneralizedRCNN(\n  (backbone): ResNet(\n    (stem): BasicStem(\n      (conv1): Conv2d(\n        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n      )\n    )\n    (res2): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n      )\n    )\n    (res3): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n    )\n    (res4): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (4): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (5): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(1024, 15, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(1024, 60, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): Res5ROIHeads(\n    (pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n      )\n    )\n    (res5): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n        )\n      )\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=2048, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=2048, out_features=4, bias=True)\n    )\n  )\n)\n"
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-4339742e158b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSemSegEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cracks_val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./output_val/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_detection_test_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cracks_val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minference_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cracks/lib/python3.6/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_train_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# For training, wrap with DDP. But don't need this for inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cracks/lib/python3.6/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mbuild_train_loader\u001b[0;34m(cls, cfg)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mOverwrite\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0myou\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0md\u001b[0m \u001b[0mlike\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_detection_train_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cracks/lib/python3.6/site-packages/detectron2/data/build.py\u001b[0m in \u001b[0;36mbuild_detection_train_loader\u001b[0;34m(cfg, mapper)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKEYPOINT_ON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mproposal_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASETS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROPOSAL_FILES_TRAIN\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOAD_PROPOSALS\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    299\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cracks/lib/python3.6/site-packages/detectron2/data/build.py\u001b[0m in \u001b[0;36mget_detection_dataset_dicts\u001b[0;34m(dataset_names, filter_empty, min_keypoints, proposal_files)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0meach\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdataset_names\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mdataset_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valer = DefaultTrainer(val_cfg) \n",
    "valer.resume_or_load(resume=False)\n",
    "evaluator = SemSegEvaluator(dataset_name='cracks_val', distributed=False, num_classes=1, output_dir='./output_val/')\n",
    "val_loader = build_detection_test_loader(val_cfg, \"cracks_val\")\n",
    "inference_on_dataset(valer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('cracks': venv)",
   "language": "python",
   "name": "python36964bitcracksvenvbf75f9d4fb5a488bbbfb5e863882f9c4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}